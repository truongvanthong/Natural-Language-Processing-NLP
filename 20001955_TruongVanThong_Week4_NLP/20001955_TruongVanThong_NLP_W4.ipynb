{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 20001955 - Trương Văn Thông\n",
    "# NLP - Week 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Các thư viện sử dụng trong bài"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "from math import log, sqrt\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 1: Sử dụng BeautifulSoup và urllib để lấy các tin tự động trên trang: https://news.google.com/news/rss\n",
    "- Viết các hàm xử lý để in ra màn hình 3 thông tin của mỗi tin: \n",
    "    - Tựa đề\n",
    "    - Đường link\n",
    "    - Ngày phát hành <br>\n",
    "`Lưu kết quả vào 1 file theo từng dòng với cấu trúc như trên.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tựa đề: White House says 'there was no indication' DeSantis would snub Biden visit after Idalia - Fox News\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiW2h0dHBzOi8vd3d3LmZveG5ld3MuY29tL3BvbGl0aWNzL3doaXRlLWhvdXNlLW5vLWluZGljYXRpb24tZGVzYW50aXMtc251Yi1iaWRlbi12aXNpdC1pZGFsaWHSAQA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 20:23:22 GMT\n",
      "\n",
      "\n",
      "Tựa đề: North Korea says it has simulated a nuclear missile attack to warn US of 'nuclear war danger' - CNN\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiZmh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8wOS8wMi9hc2lhL25vcnRoLWtvcmVhLW51Y2xlYXItbWlzc2lsZS1hdHRhY2stc3RpbXVsYXRpb24taW50bC1obmsvaW5kZXguaHRtbNIBAA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 01:18:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: RAW VIDEO | Burning Man Festival: Attendees told to shelter in place and conserve food, water & fuel - KHOU 11\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC0pSUk8xeXd3U2ZFmAEB?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 00:14:08 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Former New Mexico Governor, U.N. Ambassador Bill Richardson dies at age 75 - MSNBC\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiCzNieDNUaFJyUEkwmAEB?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 17:19:26 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Typhoon Haikui 2023 LIVE | Thousands Evacuated, Flights Cancelled As Taiwan Girds For Typhoon Haikui - CNN-News18\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC2w4UTYyU2d3Ym44mAEB?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 01:31:59 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Utah YouTuber Ruby Franke arrested on child abuse charges after malnourished son escapes home - The Independent\n",
      "Đường Link: https://news.google.com/rss/articles/CBMib2h0dHBzOi8vd3d3LmluZGVwZW5kZW50LmNvLnVrL25ld3Mvd29ybGQvYW1lcmljYXMvY3JpbWUvcnVieS1mcmFua2UtOC1wYXNzZW5nZXJzLXlvdXR1YmUtYXJyZXN0ZWQtYjI0MDM4MzguaHRtbNIBAA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 15:17:53 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Maui firefighters faced blackout conditions, limited water, extreme wind during deadly Aug. 8 Lahaina wildfire - Maui Now\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiiwFodHRwczovL21hdWlub3cuY29tLzIwMjMvMDkvMDIvbWF1aS1maXJlZmlnaHRlcnMtZmFjZWQtYmxhY2tvdXQtY29uZGl0aW9ucy1saW1pdGVkLXdhdGVyLWV4dHJlbWUtd2luZC1kdXJpbmctZGVhZGx5LWF1Zy04LWxhaGFpbmEtd2lsZGZpcmUv0gEA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 03:39:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Chester County Prison escape: Danelo Cavalcante sighted 1.5 miles away - CBS News\n",
      "Đường Link: https://news.google.com/rss/articles/CBMigQFodHRwczovL3d3dy5jYnNuZXdzLmNvbS9waGlsYWRlbHBoaWEvbmV3cy9jaGVzdGVyLWNvdW50eS1wcmlzb24tZXNjYXBlLXBlbm5zeWx2YW5pYS1kYW5lbG8tY2F2YWxjYW50ZS1zaWdodGluZy1wb2NvcHNvbi10b3duc2hpcC_SAYUBaHR0cHM6Ly93d3cuY2JzbmV3cy5jb20vYW1wL3BoaWxhZGVscGhpYS9uZXdzL2NoZXN0ZXItY291bnR5LXByaXNvbi1lc2NhcGUtcGVubnN5bHZhbmlhLWRhbmVsby1jYXZhbGNhbnRlLXNpZ2h0aW5nLXBvY29wc29uLXRvd25zaGlwLw?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 03:35:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: More rainfall on the way for Southern California - KTLA 5\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC3VmNzBzbGRfSlVZmAEB?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 02:04:02 GMT\n",
      "\n",
      "\n",
      "Tựa đề: 16-year-old student killed in shooting at Louisiana high school football game - New York Post \n",
      "Đường Link: https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vbnlwb3N0LmNvbS8yMDIzLzA5LzAyL2xvdWlzYW5hLWhpZ2gtc2Nob29sLXN0dWRlbnQta2lsbGVkLWluLWZvb3RiYWxsLWdhbWUtc2hvb3Rpbmcv0gFgaHR0cHM6Ly9ueXBvc3QuY29tLzIwMjMvMDkvMDIvbG91aXNhbmEtaGlnaC1zY2hvb2wtc3R1ZGVudC1raWxsZWQtaW4tZm9vdGJhbGwtZ2FtZS1zaG9vdGluZy9hbXAv?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 03:52:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Ukraine live briefing: Russia and Belarus disinvited from Nobel ceremony; Ukrainian billionaire detained - The Washington Post\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS93b3JsZC8yMDIzLzA5LzAzL3J1c3NpYS11a3JhaW5lLXdhci1uZXdzL9IBAA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 07:32:14 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Netanyahu says government will look into deporting migrants who rioted in Tel Aviv - The Times of Israel\n",
      "Đường Link: https://news.google.com/rss/articles/CBMicWh0dHBzOi8vd3d3LnRpbWVzb2Zpc3JhZWwuY29tL25ldGFueWFodS1zYXlzLWdvdmVybm1lbnQtd2lsbC1sb29rLWludG8tZGVwb3J0aW5nLW1pZ3JhbnRzLXdoby1yaW90ZWQtaW4tdGVsLWF2aXYv0gF1aHR0cHM6Ly93d3cudGltZXNvZmlzcmFlbC5jb20vbmV0YW55YWh1LXNheXMtZ292ZXJubWVudC13aWxsLWxvb2staW50by1kZXBvcnRpbmctbWlncmFudHMtd2hvLXJpb3RlZC1pbi10ZWwtYXZpdi9hbXAv?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 20:47:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Crimean Bridge traffic resumes after brief suspension - Russia-installed operator - Reuters\n",
      "Đường Link: https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LnJldXRlcnMuY29tL3dvcmxkL2V1cm9wZS9jcmltZWFuLWJyaWRnZS10cmFmZmljLXN1c3BlbmRlZC1ydXNzaWEtaW5zdGFsbGVkLW9wZXJhdG9yLTIwMjMtMDktMDMv0gEA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 01:10:47 GMT\n",
      "\n",
      "\n",
      "Tựa đề: India's launches first sun-studying spacecraft, Aditya-L1 - CNN\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiSmh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8wOS8wMi93b3JsZC9pbmRpYS1zdW4tcHJvYmUtYWRpdHlhLXNjbi9pbmRleC5odG1s0gEA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 00:51:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: American college student identified as passenger who went overboard from world's largest cruise ship - Fox News\n",
      "Đường Link: https://news.google.com/rss/articles/CBMidWh0dHBzOi8vd3d3LmZveG5ld3MuY29tL3dvcmxkL2FtZXJpY2FuLWNvbGxlZ2Utc3R1ZGVudC1pZGVudGlmaWVkLXBhc3Nlbmdlci13ZW50LW92ZXJib2FyZC13b3JsZHMtbGFyZ2VzdC1jcnVpc2Utc2hpcNIBeWh0dHBzOi8vd3d3LmZveG5ld3MuY29tL3dvcmxkL2FtZXJpY2FuLWNvbGxlZ2Utc3R1ZGVudC1pZGVudGlmaWVkLXBhc3Nlbmdlci13ZW50LW92ZXJib2FyZC13b3JsZHMtbGFyZ2VzdC1jcnVpc2Utc2hpcC5hbXA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 01:18:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: China's economic slowdown reverberates across Asia - Financial Times\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiP2h0dHBzOi8vd3d3LmZ0LmNvbS9jb250ZW50L2FlM2MxNDZmLTU3NTItNGQwNC04YTYyLWQ3OTU3YTg1ZjU0MtIBAA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 02:00:35 GMT\n",
      "\n",
      "\n",
      "Tựa đề: The cable cowboy has stuck a dagger into Bob Iger with Disney and ESPN on the ropes - Fortune\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiZWh0dHBzOi8vZm9ydHVuZS5jb20vMjAyMy8wOS8wMi9jaGFydGVyLXN0aWNrcy1kYWdnZXItaW50by1ib2ItaWdlci13aXRoLWRpc25leS1hbmQtZXNwbi1vbi10aGUtcm9wZXMv0gFpaHR0cHM6Ly9mb3J0dW5lLmNvbS8yMDIzLzA5LzAyL2NoYXJ0ZXItc3RpY2tzLWRhZ2dlci1pbnRvLWJvYi1pZ2VyLXdpdGgtZGlzbmV5LWFuZC1lc3BuLW9uLXRoZS1yb3Blcy9hbXAv?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 17:56:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Shibarium Wallets Hit 1M Mark Amid Ongoing Shiba Inu (SHIB) Price Challenges - BeInCrypto\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiOWh0dHBzOi8vYmVpbmNyeXB0by5jb20vc2hpYmFyaXVtLXdhbGxldHMtc2hpYmEtaW51LXByaWNlL9IBAA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 16:30:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Google Pixel 8 Pro to launch new Night Sight for videos feature and with SIM card slot globally - Notebookcheck.net\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiiwFodHRwczovL3d3dy5ub3RlYm9va2NoZWNrLm5ldC9Hb29nbGUtUGl4ZWwtOC1Qcm8tdG8tbGF1bmNoLW5ldy1OaWdodC1TaWdodC1mb3ItdmlkZW9zLWZlYXR1cmUtYW5kLXdpdGgtU0lNLWNhcmQtc2xvdC1nbG9iYWxseS43NDU3NzYuMC5odG1s0gEA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 22:27:16 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Lenovo debuts gaming glasses and portal PC handheld - TechCrunch\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wOS8wMS9sZW5vdm8tZGVidXRzLWdhbWluZy1nbGFzc2VzLWFuZC1wb3J0YWwtcGMtaGFuZGhlbGQv0gFaaHR0cHM6Ly90ZWNoY3J1bmNoLmNvbS8yMDIzLzA5LzAxL2xlbm92by1kZWJ1dHMtZ2FtaW5nLWdsYXNzZXMtYW5kLXBvcnRhbC1wYy1oYW5kaGVsZC9hbXAv?oc=5\n",
      "Ngày phát hành: Fri, 01 Sep 2023 21:45:28 GMT\n",
      "\n",
      "\n",
      "Tựa đề: More confusion reigns over what Apple will call its top-of-the-line iPhone 15 model - PhoneArena\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiQ2h0dHBzOi8vd3d3LnBob25lYXJlbmEuY29tL25ld3MvaXBob25lLTE1LXVsdHJhLW9yLXByby1tYXhfaWQxNTAzNTXSAQA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 18:43:36 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Starfield players feel “stupid” after discovering fast travel outside menus - Dexerto\n",
      "Đường Link: https://news.google.com/rss/articles/CBMidGh0dHBzOi8vd3d3LmRleGVydG8uY29tL3N0YXJmaWVsZC9zdGFyZmllbGQtcGxheWVycy1mZWVsLXN0dXBpZC1hZnRlci1kaXNjb3ZlcmluZy1mYXN0LXRyYXZlbC1vdXRzaWRlLW1lbnVzLTIyNzY0MDMv0gEA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 21:06:42 GMT\n",
      "\n",
      "\n",
      "Tựa đề: A look back at Jimmy Buffett's most memorable moments - CNN\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC2hvZUJjc2h1cV8wmAEB?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 11:42:38 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Horoscope for Sunday, September 3, 2023 - Chicago Sun-Times\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiV2h0dHBzOi8vY2hpY2Fnby5zdW50aW1lcy5jb20vMjAyMy85LzMvMjM4NDk4OTYvaG9yb3Njb3Blcy10b2RheS1zdW5kYXktc2VwdGVtYmVyLTMtMjAyM9IBAA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 05:04:09 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Meghan Markle and Prince Harry Spotted at Beyoncé's Renaissance Tour - E! NEWS\n",
      "Đường Link: https://news.google.com/rss/articles/CBMidmh0dHBzOi8vd3d3LmVvbmxpbmUuY29tL25ld3MvMTM4NDc0My9tZWdoYW4tbWFya2xlLWFuZC1wcmluY2UtaGFycnktc3BvdHRlZC1hdC1iZXlvbmNlcy1yZW5haXNzYW5jZS13b3JsZC10b3VyLWNvbmNlcnTSAQA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 00:01:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Trish Stratus receives emotional ovation: WWE Payback 2023 exclusive - WWE\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC3gtRWREekUxWnlrmAEB?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 02:00:53 GMT\n",
      "\n",
      "\n",
      "Tựa đề: AP Top 25 Takeaways: Believe the hype! Coach Prime delivers thrilling upset in debut for Colorado - The Associated Press\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiZWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL2NvYWNoLXByaW1lLWNvbG9yYWRvLXRjdS1wYWMxMi1vaGlvLXN0YXRlLTRiODIzZDI3Y2I2YThiZWFkYjQzOWE0YmQ3MjY0YmUz0gEA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 03:21:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: North Carolina vs. South Carolina Game Highlights | 2023 ACC Football - ACC Digital Network\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC0I5bG9FdnliOURrmAEB?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 03:21:41 GMT\n",
      "\n",
      "\n",
      "Tựa đề: College football scores, schedule, NCAA top 25 rankings, games: Georgia, Alabama, USC, Texas A&M in action - CBS Sports\n",
      "Đường Link: https://news.google.com/rss/articles/CBMimQFodHRwczovL3d3dy5jYnNzcG9ydHMuY29tL2NvbGxlZ2UtZm9vdGJhbGwvbmV3cy9jb2xsZWdlLWZvb3RiYWxsLXNjb3Jlcy1zY2hlZHVsZS1uY2FhLXRvcC0yNS1yYW5raW5ncy1nYW1lcy1nZW9yZ2lhLWFsYWJhbWEtdXNjLXRleGFzLWEtbS1pbi1hY3Rpb24vbGl2ZS_SAQA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 01:43:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Drew Allar impresses as No. 7 Penn State knocks off West Virginia, 38-15 - Yahoo Sports\n",
      "Đường Link: https://news.google.com/rss/articles/CBMibmh0dHBzOi8vc3BvcnRzLnlhaG9vLmNvbS9kcmV3LWFsbGFyLWltcHJlc3Nlcy1hcy1uby03LXBlbm4tc3RhdGUta25vY2tzLW9mZi13ZXN0LXZpcmdpbmlhLTM4LTE1LTAyNDg1Nzk1Ny5odG1s0gF2aHR0cHM6Ly9zcG9ydHMueWFob28uY29tL2FtcGh0bWwvZHJldy1hbGxhci1pbXByZXNzZXMtYXMtbm8tNy1wZW5uLXN0YXRlLWtub2Nrcy1vZmYtd2VzdC12aXJnaW5pYS0zOC0xNS0wMjQ4NTc5NTcuaHRtbA?oc=5\n",
      "Ngày phát hành: Sun, 03 Sep 2023 02:48:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: West Coast Falcon 9 launches 13 demonstration satellites for military mega-constellation – Spaceflight Now - Spaceflight Now\n",
      "Đường Link: https://news.google.com/rss/articles/CBMif2h0dHBzOi8vc3BhY2VmbGlnaHRub3cuY29tLzIwMjMvMDkvMDIvd2VzdC1jb2FzdC1mYWxjb24tOS1sYXVuY2hlcy0xMy1kZW1vbnN0cmF0aW9uLXNhdGVsbGl0ZXMtZm9yLW1pbGl0YXJ5LW1lZ2EtY29uc3RlbGxhdGlvbi_SAQA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 21:23:40 GMT\n",
      "\n",
      "\n",
      "Tựa đề: NASA reveals gash on moon left by crashed Russian spacecraft - Mashable\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vbWFzaGFibGUuY29tL2FydGljbGUvbHVuYS0yNS1ydXNzaWEtbW9vbi1jcmFzaC1uYXNh0gEA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 09:00:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Blue supermoon photos: See images from around the world - The Hill\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiYmh0dHBzOi8vdGhlaGlsbC5jb20vaG9tZW5ld3Mvc3BhY2UvNDE4MjA5NS1ibHVlLXN1cGVybW9vbi1waG90b3Mtc2VlLWltYWdlcy1mcm9tLWFyb3VuZC10aGUtd29ybGQv0gFmaHR0cHM6Ly90aGVoaWxsLmNvbS9ob21lbmV3cy9zcGFjZS80MTgyMDk1LWJsdWUtc3VwZXJtb29uLXBob3Rvcy1zZWUtaW1hZ2VzLWZyb20tYXJvdW5kLXRoZS13b3JsZC9hbXAv?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 01:04:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Homo floresiensis: The curious case of the hobbit and what its remarkable discovery taught us - CNN\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiYWh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8wOS8wMi9hc2lhL2hvbW8tZmxvcmVzaWVuc2lzLWhvYmJpdC1kaXNjb3ZlcnktYW5uaXZlcnNhcnktc2NuL2luZGV4Lmh0bWzSAQA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 11:00:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: New COVID-19 Variant Expands Internationally - VOA Learning English\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vbGVhcm5pbmdlbmdsaXNoLnZvYW5ld3MuY29tL2EvbmV3LWNvdmlkLTE5LXZhcmlhbnQtZXhwYW5kcy1pbnRlcm5hdGlvbmFsbHkvNzI0Nzk5MC5odG1s0gFhaHR0cHM6Ly9sZWFybmluZ2VuZ2xpc2gudm9hbmV3cy5jb20vYW1wL25ldy1jb3ZpZC0xOS12YXJpYW50LWV4cGFuZHMtaW50ZXJuYXRpb25hbGx5LzcyNDc5OTAuaHRtbA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 21:55:23 GMT\n",
      "\n",
      "\n",
      "Tựa đề: New Jersey nursing homes battle COVID-19 surge as US cases rise - Insider\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiZ2h0dHBzOi8vd3d3Lmluc2lkZXIuY29tL2NvdmlkLTE5LWNvcm9uYXZpcnVzLXBhbmRlbWljLXZhcmlhbnRzLW51cnNpbmctaG9tZXMtb3V0YnJlYWstbmV3LWplcnNleS0yMDIzLTnSAWtodHRwczovL3d3dy5pbnNpZGVyLmNvbS9jb3ZpZC0xOS1jb3JvbmF2aXJ1cy1wYW5kZW1pYy12YXJpYW50cy1udXJzaW5nLWhvbWVzLW91dGJyZWFrLW5ldy1qZXJzZXktMjAyMy05P2FtcA?oc=5\n",
      "Ngày phát hành: Sat, 02 Sep 2023 17:06:00 GMT\n",
      "\n",
      "\n",
      "Tựa đề: West Nile virus detected in Cranston mosquitoes - WPRI\n",
      "Đường Link: https://news.google.com/rss/articles/CCAiC3NPaVlrZ1pqNlAwmAEB?oc=5\n",
      "Ngày phát hành: Fri, 01 Sep 2023 22:32:44 GMT\n",
      "\n",
      "\n",
      "Tựa đề: Over-the-counter Narcan: Availability, cost, and how to administer - Vox.com\n",
      "Đường Link: https://news.google.com/rss/articles/CBMiYGh0dHBzOi8vd3d3LnZveC5jb20vMjM4NTU1NDMvbmFyY2FuLW5hbG94b25lLW92ZXItdGhlLWNvdW50ZXItd2FsZ3JlZW5zLWNvc3QtdXNlLW92ZXJkb3NlLW9waW9pZNIBbWh0dHBzOi8vd3d3LnZveC5jb20vcGxhdGZvcm0vYW1wLzIzODU1NTQzL25hcmNhbi1uYWxveG9uZS1vdmVyLXRoZS1jb3VudGVyLXdhbGdyZWVucy1jb3N0LXVzZS1vdmVyZG9zZS1vcGlvaWQ?oc=5\n",
      "Ngày phát hành: Fri, 01 Sep 2023 15:34:43 GMT\n",
      "\n",
      "\n",
      "Dữ liệu đã được lưu vào tệp: rss_feed.txt\n"
     ]
    }
   ],
   "source": [
    "def fetch_rss_data(url):\n",
    "    try:\n",
    "        news_list = []\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            rss_data = response.read()\n",
    "\n",
    "        soup = BeautifulSoup(rss_data, 'xml')\n",
    "        items = soup.find_all('item')\n",
    "\n",
    "        for item in items:\n",
    "            title = item.title.text if item.title else \"Không có tiêu đề\"\n",
    "            link = item.link.text if item.link else \"Không có đường link\"\n",
    "            pub_date = item.pubDate.text if item.pubDate else \"Không có ngày phát hành\"\n",
    "            \n",
    "            news_list.append((title, link, pub_date))\n",
    "            print(f\"Tựa đề: {title}\")\n",
    "            print(f\"Đường Link: {link}\")\n",
    "            print(f\"Ngày phát hành: {pub_date}\")\n",
    "            print(\"\\n\")\n",
    "\n",
    "        return news_list\n",
    "\n",
    "    except urllib.error.URLError as e:\n",
    "        print(f\"Lỗi kết nối mạng: {e}\")\n",
    "        return None\n",
    "    except Exception as ex:\n",
    "        print(f\"Lỗi không xác định: {ex}\")\n",
    "        return None\n",
    "\n",
    "def save_to_file(news_list, filename=\"rss_feed.txt\"):\n",
    "    try:\n",
    "        with open(filename, \"w\", encoding=\"utf-8\") as file:\n",
    "            for title, link, pub_date in news_list:\n",
    "                file.write(f\"Tựa đề: {title}\\n\")\n",
    "                file.write(f\"Đường Link: {link}\\n\")\n",
    "                file.write(f\"Ngày phát hành: {pub_date}\\n\\n\")\n",
    "\n",
    "        print(f\"Dữ liệu đã được lưu vào tệp: {filename}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Lỗi không xác định: {ex}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://news.google.com/news/rss\"\n",
    "    news_list = fetch_rss_data(url)\n",
    "    if news_list:\n",
    "        save_to_file(news_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 2: Cho dữ liệu như sau:\n",
    "`Data = [{'Thanh pho': 'Hai Phong', 'Nhiet do': 32}, {'Thanh pho': 'Da Nang',\n",
    "'Nhiet do': 29}, {'Thanh pho': 'Can Tho', 'Nhiet do': 34}]`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hãy chuyển dữ liệu trên thành số sử dụng thư viện DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu sau khi đã được chuyển đổi sử dụng thư viện DictVectorizer:\n",
      "[[32.  0.  0.  1.]\n",
      " [29.  0.  1.  0.]\n",
      " [34.  1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Dữ liệu ban đầu\n",
    "Data = [\n",
    "    {'Thanh pho': 'Hai Phong', 'Nhiet do': 32},\n",
    "    {'Thanh pho': 'Da Nang', 'Nhiet do': 29}, \n",
    "    {'Thanh pho': 'Can Tho', 'Nhiet do': 34}\n",
    "]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec_data = vec.fit_transform(Data).toarray()\n",
    "\n",
    "print(\"Dữ liệu sau khi đã được chuyển đổi sử dụng thư viện DictVectorizer:\")\n",
    "print(vec_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In ra tên các feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tên các feature:\n",
      "['Nhiet do' 'Thanh pho=Can Tho' 'Thanh pho=Da Nang' 'Thanh pho=Hai Phong']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tên các feature:\")\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 3: Cho dữ liệu sau:\n",
    "data= [{'word-2': 'con',\n",
    "        'pos-2': 'DT',\n",
    "        'word-1': 'mèo',\n",
    "        'pos-1': 'NN',\n",
    "        'word+1': 'trên',\n",
    "        'pos+1': 'PP', }]\n",
    "\n",
    "1. Hãy chuyển dữ liệu trên thành số sử dụng thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dữ liệu sau khi đã được chuyển đổi:\n",
      "[[1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "data= [{'word-2': 'con',\n",
    "        'pos-2': 'DT',\n",
    "        'word-1': 'mèo',\n",
    "        'pos-1': 'NN',\n",
    "        'word+1': 'trên',\n",
    "        'pos+1': 'PP', }]\n",
    "\n",
    "vec = DictVectorizer()\n",
    "vec_data = vec.fit_transform(data).toarray()\n",
    "\n",
    "print(\"Dữ liệu sau khi đã được chuyển đổi:\")\n",
    "print(vec_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In ra tên các feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tên các feature:\n",
      "['pos+1=PP' 'pos-1=NN' 'pos-2=DT' 'word+1=trên' 'word-1=mèo' 'word-2=con']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tên các feature:\")\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 4: Cho dữ liệu sau:\n",
    "dulieu=[\n",
    "‘Hôm_nay tôi đi_học',\n",
    "' Hôm_nay tôi đi_học ở trường',\n",
    "' Hôm_nay tôi nghỉ ở nhà',\n",
    "'Hôm_nay tôi có đi_học không?']\n",
    "\n",
    "1. Hãy chuyển dữ liệu trên thành số sử dụng thư viện CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chuyển dữ liệu thành số sử dụng thư viện CountVectorizer:\n",
      "[[0 1 0 0 0 0 1 1]\n",
      " [0 1 0 0 0 1 1 1]\n",
      " [0 1 0 1 1 0 1 0]\n",
      " [1 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "dulieu=[\n",
    "'Hôm_nay tôi đi_học',\n",
    "' Hôm_nay tôi đi_học ở trường',\n",
    "' Hôm_nay tôi nghỉ ở nhà',\n",
    "'Hôm_nay tôi có đi_học không?']\n",
    "\n",
    "vec = CountVectorizer()\n",
    "vec_data = vec.fit_transform(dulieu).toarray()\n",
    "\n",
    "print(\"Chuyển dữ liệu thành số sử dụng thư viện CountVectorizer:\")\n",
    "print(vec_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In ra tên của các feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tên các feature:\n",
      "['có' 'hôm_nay' 'không' 'nghỉ' 'nhà' 'trường' 'tôi' 'đi_học']\n"
     ]
    }
   ],
   "source": [
    "print(\"Tên các feature:\")\n",
    "print(vec.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Vào một câu bất kỳ, in ra vector giá trị số của câu đó dựa trên dữ liệu ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector của câu 'Hôm_nay tôi có đi_học không?': \n",
      "[[1 1 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Câu bất kỳ\n",
    "cau = \"Hôm_nay tôi có đi_học không?\"\n",
    "\n",
    "# Chuyển câu thành vector\n",
    "vec_cau = vec.transform([cau]).toarray()\n",
    "\n",
    "print(\"Vector của câu '{}': \".format(cau))\n",
    "print(vec_cau)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 5: Cho dữ liệu sau:\n",
    "`Tôi là sinh viên trường Đại học Công nghiệp thành phố Hồ Chí Minh` <br>\n",
    "Chuyển câu trên thành unigram, bigram và trigram sử dụng thư viện có sẵn và in\n",
    "kết quả ra màn hình."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('Tôi',), ('là',), ('sinh',), ('viên',), ('trường',), ('Đại',), ('học',), ('Công',), ('nghiệp',), ('thành',), ('phố',), ('Hồ',), ('Chí',), ('Minh',)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Bigrams: [('Tôi', 'là'), ('là', 'sinh'), ('sinh', 'viên'), ('viên', 'trường'), ('trường', 'Đại'), ('Đại', 'học'), ('học', 'Công'), ('Công', 'nghiệp'), ('nghiệp', 'thành'), ('thành', 'phố'), ('phố', 'Hồ'), ('Hồ', 'Chí'), ('Chí', 'Minh')]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Trigrams: [('Tôi', 'là', 'sinh'), ('là', 'sinh', 'viên'), ('sinh', 'viên', 'trường'), ('viên', 'trường', 'Đại'), ('trường', 'Đại', 'học'), ('Đại', 'học', 'Công'), ('học', 'Công', 'nghiệp'), ('Công', 'nghiệp', 'thành'), ('nghiệp', 'thành', 'phố'), ('thành', 'phố', 'Hồ'), ('phố', 'Hồ', 'Chí'), ('Hồ', 'Chí', 'Minh')]\n"
     ]
    }
   ],
   "source": [
    "# Câu ban đầu\n",
    "sentence = \"Tôi là sinh viên trường Đại học Công nghiệp thành phố Hồ Chí Minh\"\n",
    "\n",
    "# Tokenize câu thành các từ\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Tạo unigram\n",
    "unigrams = list(ngrams(words, 1))\n",
    "\n",
    "# Tạo bigram\n",
    "bigrams = list(ngrams(words, 2))\n",
    "\n",
    "# Tạo trigram\n",
    "trigrams = list(ngrams(words, 3))\n",
    "\n",
    "\n",
    "print('Unigrams:', unigrams)\n",
    "print(\"---\"*50)\n",
    "print('Bigrams:', bigrams)\n",
    "print(\"---\"*50)\n",
    "print('Trigrams:', trigrams)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 6: Cho dữ liệu huấn luyện như sau:\n",
    "```HTML\n",
    "<s> tôi là IUH </s>\n",
    "<s> IUH là tôi </s>\n",
    "<s> IUH tôi học ở </s>\n",
    "<s> IUH tôi đã học ở </s>\n",
    "<s> tôi đã học ở IUH sao </s>\n",
    "```\n",
    "Giả sử chúng ta sử dụng bigram language model dựa trên dữ liệu đã huấn luyện ở\n",
    "trên."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(tôi|<s>) = 0.4\n",
      "P(là|tôi) = 0.2\n",
      "P(IUH|là) = 0.5\n",
      "P(</s>|IUH) = 0.2\n",
      "P(IUH|<s>) = 0.6\n",
      "P(là|IUH) = 0.2\n",
      "P(tôi|là) = 0.5\n",
      "P(</s>|tôi) = 0.2\n",
      "P(tôi|IUH) = 0.4\n",
      "P(học|tôi) = 0.2\n",
      "P(ở|học) = 1.0\n",
      "P(</s>|ở) = 0.6666666666666666\n",
      "P(đã|tôi) = 0.4\n",
      "P(học|đã) = 1.0\n",
      "P(IUH|ở) = 0.3333333333333333\n",
      "P(sao|IUH) = 0.2\n",
      "P(</s>|sao) = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Dữ liệu huấn luyện\n",
    "sentences = [\n",
    "    \"<s> tôi là IUH </s>\",\n",
    "    \"<s> IUH là tôi </s>\",\n",
    "    \"<s> IUH tôi học ở </s>\",\n",
    "    \"<s> IUH tôi đã học ở </s>\",\n",
    "    \"<s> tôi đã học ở IUH sao </s>\"\n",
    "]\n",
    "\n",
    "# Tokenization và Đếm Bigram\n",
    "bigrams = []\n",
    "unigrams = []\n",
    "for sentence in sentences:\n",
    "    tokens = sentence.split()\n",
    "    unigrams.extend(tokens)\n",
    "    bigrams.extend([(tokens[i], tokens[i + 1]) for i in range(len(tokens) - 1)])\n",
    "\n",
    "bigram_counts = Counter(bigrams)\n",
    "unigram_counts = Counter(unigrams)\n",
    "\n",
    "# Tính Xác Suất\n",
    "bigram_probabilities = defaultdict(float)\n",
    "for (w1, w2), count in bigram_counts.items():\n",
    "    bigram_probabilities[(w1, w2)] = count / unigram_counts[w1]\n",
    "\n",
    "# In kết quả\n",
    "for (w1, w2), prob in bigram_probabilities.items():\n",
    "    print(f\"P({w2}|{w1}) = {prob}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Hãy cho biết từ có thể xuất hiện tiếp theo theo model huấn luyện ở trên trong các câu sau là gì?\n",
    "```HTML\n",
    "(1) <s> IUH . . .\n",
    "(2) <s> IUH tôi học . . .\n",
    "(3) <s> IUH tôi là IUH . . .\n",
    "(4) <s> tôi đã học . . .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1) '<s> IUH ...', từ có thể xuất hiện tiếp theo là: '</s>'\n",
      "(2) '<s> IUH tôi học ...', từ có thể xuất hiện tiếp theo là: 'ở'\n",
      "(3) '<s> IUH tôi là IUH ...', từ có thể xuất hiện tiếp theo là: '</s>'\n",
      "(4) '<s> tôi đã học ...', từ có thể xuất hiện tiếp theo là: 'ở'\n"
     ]
    }
   ],
   "source": [
    "# Dự đoán từ tiếp theo\n",
    "def predict_next_word(sentence):\n",
    "    tokens = sentence.split()\n",
    "    last_token = tokens[-1]\n",
    "    possible_next_words = [w2 for (w1, w2) in bigram_probabilities.keys() if w1 == last_token]\n",
    "    \n",
    "    if not possible_next_words:\n",
    "        return \"Không có từ nào có thể xuất hiện tiếp theo.\"\n",
    "    \n",
    "    # Trả về từ có xác suất xuất hiện cao nhất (trong trường hợp này, tất cả đều có xác suất bằng nhau)\n",
    "    return possible_next_words[0]  # Điều này giả định rằng tất cả các từ có xác suất xuất hiện tiếp theo bằng nhau\n",
    "\n",
    "# Test\n",
    "test_sentences = [\n",
    "    \"<s> IUH\",\n",
    "    \"<s> IUH tôi học\",\n",
    "    \"<s> IUH tôi là IUH\",\n",
    "    \"<s> tôi đã học\"\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(test_sentences, start=1):\n",
    "    next_word = predict_next_word(sentence)\n",
    "    print(f\"({i}) '{sentence} ...', từ có thể xuất hiện tiếp theo là: '{next_word}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Trong các câu sau, câu nào cho xác suất cao nhất với mô hình đã huấn luyện ở\n",
    "trên?\n",
    "```HTML\n",
    "(5) <s> IUH tôi đã tôi học </s>\n",
    "(6) <s> IUH tôi là </s>\n",
    "(7) <s> tôi đã học IUH tôi là </s>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xác suất của câu '<s> IUH tôi đã tôi học </s>' là: 0\n",
      "Xác suất của câu '<s> IUH tôi là </s>' là: 0\n",
      "Xác suất của câu '<s> tôi đã học IUH tôi là </s>' là: 0\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_probability(sentence, bigram_probabilities):\n",
    "    tokens = sentence.split()\n",
    "    log_prob = 0.0  # Sử dụng log để tính\n",
    "    \n",
    "    for i in range(1, len(tokens)):\n",
    "        w1, w2 = tokens[i - 1], tokens[i]\n",
    "        prob = bigram_probabilities.get((w1, w2), 0)  # 0 nếu bigram không tồn tại trong tập dữ liệu\n",
    "        if prob == 0:  # Nếu bigram không tồn tại, câu đó có xác suất là 0\n",
    "            return 0\n",
    "        \n",
    "        log_prob += math.log(prob)\n",
    "    \n",
    "    return math.exp(log_prob)  # Convert lại từ log xác suất sang xác suất\n",
    "\n",
    "# Các câu cần tính xác suất\n",
    "test_sentences = [\n",
    "    \"<s> IUH tôi đã tôi học </s>\",\n",
    "    \"<s> IUH tôi là </s>\",\n",
    "    \"<s> tôi đã học IUH tôi là </s>\"\n",
    "]\n",
    "\n",
    "# Tính xác suất và in ra\n",
    "for sentence in test_sentences:\n",
    "    prob = calculate_sentence_probability(sentence, bigram_probabilities)\n",
    "    print(f\"Xác suất của câu '{sentence}' là: {prob}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Hãy tính complexity của câu sau:\n",
    "```HTML\n",
    "<s> tôi đã học ở IUH\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complexity của câu '<s> tôi đã học ở IUH' là: 1.6299222137747946\n"
     ]
    }
   ],
   "source": [
    "def calculate_sentence_complexity(sentence, bigram_probabilities):\n",
    "    tokens = sentence.split()\n",
    "    N = len(tokens)\n",
    "    log_prob = 0.0\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        w1, w2 = tokens[i - 1], tokens[i]\n",
    "        prob = bigram_probabilities.get((w1, w2), 0)\n",
    "        \n",
    "        if prob == 0:\n",
    "            return float('inf')  # return infinity nếu câu có xác suất là 0\n",
    "        \n",
    "        log_prob += math.log(prob)\n",
    "        \n",
    "    complexity = math.exp(-log_prob / N)\n",
    "    \n",
    "    return complexity\n",
    "\n",
    "# Câu cần tính complexity\n",
    "sentence = \"<s> tôi đã học ở IUH\"\n",
    "\n",
    "# Tính complexity và in ra\n",
    "complexity = calculate_sentence_complexity(sentence, bigram_probabilities)\n",
    "print(f\"Complexity của câu '{sentence}' là: {complexity}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Tính xác suất bigram dựa trên mô hình huấn luyện ở trên cho các từ sau:\n",
    "```HTML\n",
    "P(học |<s>)\n",
    "P(học | IUH)\n",
    "P(IUH|<s>)\n",
    "P(IUH| học)\n",
    "P(tôi|IUH)\n",
    "P(tôi|học)\n",
    "P(học|tôi)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(học | <s>) = 0.0\n",
      "P(học | IUH) = 0.0\n",
      "P(IUH | <s>) = 0.6\n",
      "P(IUH | học) = 0.0\n",
      "P(tôi | IUH) = 0.4\n",
      "P(tôi | học) = 0.0\n",
      "P(học | tôi) = 0.2\n"
     ]
    }
   ],
   "source": [
    "# Tính xác suất bigram\n",
    "def calc_bigram_prob(w1, w2):\n",
    "    return bigram_counts.get((w1, w2), 0) / unigram_counts.get(w1, 0)\n",
    "\n",
    "# Tính và in ra các xác suất bigram cần tìm\n",
    "bigrams_to_check = [\n",
    "    (\"<s>\", \"học\"),\n",
    "    (\"IUH\", \"học\"),\n",
    "    (\"<s>\", \"IUH\"),  \n",
    "    (\"học\", \"IUH\"),\n",
    "    (\"IUH\", \"tôi\"), \n",
    "    (\"học\", \"tôi\"),  \n",
    "    (\"tôi\", \"học\")   \n",
    "]\n",
    "\n",
    "for w1, w2 in bigrams_to_check:\n",
    "    prob = calc_bigram_prob(w1, w2)\n",
    "    print(f\"P({w2} | {w1}) = {prob}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Hãy tính xác suất của câu sau:\n",
    "```HTML\n",
    "(8) <s> tôi đã học ở IUH sao\n",
    "(9) <s> IUH tôi đã học\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xác suất của câu '<s> tôi đã học ở IUH sao' là: 0.01066666666666667\n",
      "Xác suất của câu '<s> IUH tôi đã học' là: 0.096\n"
     ]
    }
   ],
   "source": [
    "# Tính xác suất của một câu\n",
    "def calc_sentence_prob(sentence, unigram_counts, bigram_counts):\n",
    "    words = sentence.split()\n",
    "    prob = 1.0\n",
    "    for w1, w2 in zip(words, words[1:]):\n",
    "        bigram_prob = bigram_counts.get((w1, w2), 0) / unigram_counts.get(w1, 0)\n",
    "        prob *= bigram_prob\n",
    "        if bigram_prob == 0:\n",
    "            return 0\n",
    "    return prob\n",
    "\n",
    "# Các câu cần tính xác suất\n",
    "sentences_to_check = [\n",
    "    \"<s> tôi đã học ở IUH sao\",\n",
    "    \"<s> IUH tôi đã học\"\n",
    "]\n",
    "\n",
    "# Tính và in ra xác suất\n",
    "for sentence in sentences_to_check:\n",
    "    prob = calc_sentence_prob(sentence, unigram_counts, bigram_counts)\n",
    "    print(f\"Xác suất của câu '{sentence}' là: {prob}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bài 7: Cho dữ liệu như ở bài 4:\n",
    "dulieu=[\n",
    "‘Hôm_nay tôi đi_học',\n",
    "' Hôm_nay tôi đi_học ở trường',\n",
    "' Hôm_nay tôi nghỉ ở nhà',\n",
    "'Hôm_nay tôi có đi_học không?'] <br><br>\n",
    "Với các định nghĩa như sau về TF-IDF: <br><br>\n",
    "Tf means **term-frequency** while tf–idf means term-frequency times **inverse document-frequency**:\n",
    "$$tf-idf(t, d) = tf(t, d) * log(N/(df + 1))$$\n",
    "✓ *tf(t,d) = count of t in d / number of words in d* <br>\n",
    "✓ *df(t) = occurrence of t in documents*<br>\n",
    "✓ *idf(t) = log(N/(df + 1))*<br>\n",
    "• *t — term (word), d — document (set of words)*<br>\n",
    "• *n — count of corpus, corpus — the total document set*<br><br>\n",
    "Kết quả sẽ được chuẩn hóa bằng công thức Euclidean: <br>\n",
    "\n",
    "$$v_{norm} = \\frac{v}{\\left \\| v \\right \\|_2} = \\frac{v}{\\sqrt{v_1^2 + v_2^2 + ... + v_n^2}}$$\n",
    "\n",
    "<br>Hãy tự tính `TF-IDF` của văn bản đã cho theo các công thức ở trên"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dữ liệu\n",
    "dulieu = [\n",
    "    'Hôm_nay tôi đi_học',\n",
    "    'Hôm_nay tôi đi_học ở trường',\n",
    "    'Hôm_nay tôi nghỉ ở nhà',\n",
    "    'Hôm_nay tôi có đi_học không'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính Term Frequency (TF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF:\n",
      "{0: {'Hôm_nay': 0.3333333333333333, 'tôi': 0.3333333333333333, 'đi_học': 0.3333333333333333}, 1: {'Hôm_nay': 0.2, 'tôi': 0.2, 'đi_học': 0.2, 'ở': 0.2, 'trường': 0.2}, 2: {'Hôm_nay': 0.2, 'tôi': 0.2, 'nghỉ': 0.2, 'ở': 0.2, 'nhà': 0.2}, 3: {'Hôm_nay': 0.2, 'tôi': 0.2, 'có': 0.2, 'đi_học': 0.2, 'không': 0.2}}\n"
     ]
    }
   ],
   "source": [
    "# Tính TF\n",
    "tf = {}\n",
    "for idx, doc in enumerate(dulieu):\n",
    "    word_count = Counter(doc.split())\n",
    "    total_words = len(doc.split())\n",
    "    tf[idx] = {word: count / total_words for word, count in word_count.items()}\n",
    "    \n",
    "print(\"TF:\")\n",
    "print(tf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tính Document Frequency (DF) và Inverse Document Frequency (IDF):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'Hôm_nay': -0.07438118377140324, 'tôi': -0.07438118377140324, 'đi_học': 0.0}, 1: {'Hôm_nay': -0.044628710262841945, 'tôi': -0.044628710262841945, 'đi_học': 0.0, 'ở': 0.05753641449035617, 'trường': 0.13862943611198905}, 2: {'Hôm_nay': -0.044628710262841945, 'tôi': -0.044628710262841945, 'nghỉ': 0.13862943611198905, 'ở': 0.05753641449035617, 'nhà': 0.13862943611198905}, 3: {'Hôm_nay': -0.044628710262841945, 'tôi': -0.044628710262841945, 'có': 0.13862943611198905, 'đi_học': 0.0, 'không': 0.13862943611198905}}\n"
     ]
    }
   ],
   "source": [
    "# Tính DF\n",
    "df = Counter()\n",
    "for doc in dulieu:\n",
    "    for word in set(doc.split()):\n",
    "        df[word] += 1\n",
    "\n",
    "# Tính IDF và TF-IDF\n",
    "N = len(dulieu)\n",
    "idf = {word: log(N / (df[word] + 1)) for word in df.keys()}\n",
    "# Tính TF-IDF\n",
    "tf_idf = {}\n",
    "for idx, doc in tf.items():\n",
    "    tf_idf[idx] = {word: tf_word * idf[word] for word, tf_word in doc.items()}\n",
    "\n",
    "print(tf_idf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chuẩn hóa TF-IDF (Euclidean Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: {'Hôm_nay': -0.7071067811865476, 'tôi': -0.7071067811865476, 'đi_học': 0.0}\n",
      "Document 2: {'Hôm_nay': -0.27408992070678545, 'tôi': -0.27408992070678545, 'đi_học': 0.0, 'ở': 0.35336336615008035, 'trường': 0.851401058372011}\n",
      "Document 3: {'Hôm_nay': -0.2086955305784808, 'tôi': -0.2086955305784808, 'nghỉ': 0.6482675289694743, 'ở': 0.269055334087166, 'nhà': 0.6482675289694743}\n",
      "Document 4: {'Hôm_nay': -0.21668588308533965, 'tôi': -0.21668588308533965, 'có': 0.673087830874639, 'đi_học': 0.0, 'không': 0.673087830874639}\n"
     ]
    }
   ],
   "source": [
    "# Chuẩn hóa TF-IDF\n",
    "normalized_tf_idf = {}\n",
    "for idx, doc in tf_idf.items():\n",
    "    norm = sqrt(sum([value ** 2 for value in doc.values()]))\n",
    "    normalized_tf_idf[idx] = {word: value / norm for word, value in doc.items()}\n",
    "\n",
    "# In kết quả\n",
    "for idx, doc in normalized_tf_idf.items():\n",
    "    print(f\"Document {idx+1}: {doc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
